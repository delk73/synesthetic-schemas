---
version: v0.7.3
lastReviewed: 2025-10-12
owner: delk73
---

# Notes: Beyond Natural Language in XAI

## Context
Finzel et al. (2021) and Lakkaraju et al. (2022) frame explainability as an intrinsically social and interactive process of **natural language communication**.

## Critique
- **Reductionism**: Natural language explanations collapse multidimensional phenomena into linear, symbolic strings.  
- **Perceptual Gap**: Real-time dynamics (e.g., modulation curves, haptic feedback, visual flows) are not adequately conveyed in text.  
- **Bias & Ambiguity**: Language carries cultural and interpretive baggage that distorts or excludes shared understanding.  
- **Exclusion**: A language-first stance privileges the literate, academic, and culturally aligned. It marginalizes users who rely on sensory, embodied, or non-linguistic modes of comprehension.

## Alternative: Sensory Grounding
- Explanations should be **experienced**: visualized, sounded, felt.  
- **Multimodal feedback loops** make system state directly legible.  
- Language may remain as companion framing, but not as the **primary substrate**.  

## Design Goal
- **Multisensory Redundancy**: Explanations should span vision, audition, and haptics so understanding is reinforced across channels.  
- **Adaptive Abstraction**: Mappings should be tunable per user, allowing reliance on the sensory mode that best grounds comprehension.  
- **Inclusion**: Natural language remains part of the stack, but as one redundant pathway among many, not the sole substrate.  

## Literature Gap
Prior work on algorithmic recourse (Poyiadzi et al. 2020; Karimi, Schölkopf, and Valera 2021) and formal dialogue protocols (Walton 2007, 2011, 2016; Madumal et al. 2019; Schneider and Handali 2019) establishes theoretical models for explanation as communication. Yet these remain **abstract, one-sided, and difficult to operationalize**. Practical adoption remains limited, with only narrow tool implementations (Kuźba and Biecek 2020; Malandri et al. 2023).  

Synesthetic directly addresses this gap by grounding explanation in **perceptual experience** rather than abstract schema. Explanations become **embodied, multimodal interactions** — visual, auditory, and haptic — enabling real-time, bidirectional dialogue between human and system. Where natural language dialogues provide only partial personalization, Synesthetic achieves inclusivity and fidelity by making explanations **experienced, not just described**.  

## Position
Explainability is best approached as **embodied communication**, where explanation = experience.  
Natural language is a *layer* in a redundant, multimodal system, not the *core medium*.  

---

### Tagline
**Explainability you experience.**

### Stance (for external use)
Explainability shouldn’t stop at words. True inclusivity and fidelity require **multisensory grounding**—visual, auditory, and haptic channels that make system behavior directly perceivable. Explanations are experienced, not just described.  

> See also: Positioning Blurb in `docs/README.md`
